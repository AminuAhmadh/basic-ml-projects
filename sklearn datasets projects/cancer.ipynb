{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# the dataset\r\n",
    "from sklearn.datasets import load_breast_cancer\r\n",
    "\r\n",
    "# classification algorithms to work with\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "# some useful libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# load the dataset\r\n",
    "cancer = load_breast_cancer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# type\r\n",
    "type(cancer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# keys\r\n",
    "cancer.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# the actual data\r\n",
    "X = cancer.data\r\n",
    "\r\n",
    "# the labels\r\n",
    "y = cancer.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# names of the labels\r\n",
    "cancer.target_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# shape of the data\r\n",
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "np.bincount(y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([212, 357], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "212 'malignant' and 357 'benign'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# any missing values?\r\n",
    "pd.isnull(X).sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# split the data into training and test set\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# instantiate the classes without any params tunning\r\n",
    "dtc = DecisionTreeClassifier(random_state=0)\r\n",
    "svc = SVC(random_state=0)\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rfc = RandomForestClassifier(random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# train the classifiers\r\n",
    "dtc.fit(X_train, y_train)\r\n",
    "svc.fit(X_train, y_train)\r\n",
    "knn.fit(X_train, y_train)\r\n",
    "rfc.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# predict and store the predictions\r\n",
    "dtc_preds = dtc.predict(X_test)\r\n",
    "svc_preds = svc.predict(X_test)\r\n",
    "knn_preds = knn.predict(X_test)\r\n",
    "rfc_preds = rfc.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(f'DTC Accuracy: {np.mean(dtc_preds == y_test)}')\r\n",
    "print(f'SVC Accuracy: {np.mean(svc_preds == y_test)}')\r\n",
    "print(f'KNN Accuracy: {knn.score(X_test, y_test)}')\r\n",
    "print(f'RFC Accuracy: {rfc.score(X_test, y_test)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DTC Accuracy: 0.8811188811188811\n",
      "SVC Accuracy: 0.9370629370629371\n",
      "KNN Accuracy: 0.9370629370629371\n",
      "RFC Accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Winner? : RANDOM FOREST CLASSIFIER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "SGDClassifier??"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hinge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'optimal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSource:\u001b[0m        \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSGDClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "\n",
      "    This estimator implements regularized linear models with stochastic\n",
      "    gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "    each sample at a time and the model is updated along the way with a\n",
      "    decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "    (online/out-of-core) learning via the `partial_fit` method.\n",
      "    For best results using the default learning rate schedule, the data should\n",
      "    have zero mean and unit variance.\n",
      "\n",
      "    This implementation works with data represented as dense or sparse arrays\n",
      "    of floating point values for the features. The model it fits can be\n",
      "    controlled with the loss parameter; by default, it fits a linear support\n",
      "    vector machine (SVM).\n",
      "\n",
      "    The regularizer is a penalty added to the loss function that shrinks model\n",
      "    parameters towards the zero vector using either the squared euclidean norm\n",
      "    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "    parameter update crosses the 0.0 value because of the regularizer, the\n",
      "    update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "    online feature selection.\n",
      "\n",
      "    Read more in the :ref:`User Guide <sgd>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : str, default='hinge'\n",
      "        The loss function to be used. Defaults to 'hinge', which gives a\n",
      "        linear SVM.\n",
      "\n",
      "        The possible options are 'hinge', 'log', 'modified_huber',\n",
      "        'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      "        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "\n",
      "        The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "        'modified_huber' is another smooth loss that brings tolerance to\n",
      "        outliers as well as probability estimates.\n",
      "        'squared_hinge' is like hinge but is quadratically penalized.\n",
      "        'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "        The other losses are designed for regression but can be useful in\n",
      "        classification as well; see\n",
      "        :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "\n",
      "        More details about the losses formulas can be found in the\n",
      "        :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "\n",
      "    penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "        The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "        which is the standard regularizer for linear SVM models. 'l1' and\n",
      "        'elasticnet' might bring sparsity to the model (feature selection)\n",
      "        not achievable with 'l2'.\n",
      "\n",
      "    alpha : float, default=0.0001\n",
      "        Constant that multiplies the regularization term. The higher the\n",
      "        value, the stronger the regularization.\n",
      "        Also used to compute the learning rate when set to `learning_rate` is\n",
      "        set to 'optimal'.\n",
      "\n",
      "    l1_ratio : float, default=0.15\n",
      "        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "        Only used if `penalty` is 'elasticnet'.\n",
      "\n",
      "    fit_intercept : bool, default=True\n",
      "        Whether the intercept should be estimated or not. If False, the\n",
      "        data is assumed to be already centered.\n",
      "\n",
      "    max_iter : int, default=1000\n",
      "        The maximum number of passes over the training data (aka epochs).\n",
      "        It only impacts the behavior in the ``fit`` method, and not the\n",
      "        :meth:`partial_fit` method.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    tol : float, default=1e-3\n",
      "        The stopping criterion. If it is not None, training will stop\n",
      "        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "        epochs.\n",
      "        Convergence is checked against the training loss or the\n",
      "        validation loss depending on the `early_stopping` parameter.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    shuffle : bool, default=True\n",
      "        Whether or not the training data should be shuffled after each epoch.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        The verbosity level.\n",
      "\n",
      "    epsilon : float, default=0.1\n",
      "        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "        For 'huber', determines the threshold at which it becomes less\n",
      "        important to get the prediction exactly right.\n",
      "        For epsilon-insensitive, any differences between the current prediction\n",
      "        and the correct label are ignored if they are less than this threshold.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        The number of CPUs to use to do the OVA (One Versus All, for\n",
      "        multi-class problems) computation.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    learning_rate : str, default='optimal'\n",
      "        The learning rate schedule:\n",
      "\n",
      "        - 'constant': `eta = eta0`\n",
      "        - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "          where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "        - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "        - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "          Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "          training loss by tol or fail to increase validation score by tol if\n",
      "          early_stopping is True, the current learning rate is divided by 5.\n",
      "\n",
      "            .. versionadded:: 0.20\n",
      "                Added 'adaptive' option\n",
      "\n",
      "    eta0 : double, default=0.0\n",
      "        The initial learning rate for the 'constant', 'invscaling' or\n",
      "        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "        the default schedule 'optimal'.\n",
      "\n",
      "    power_t : double, default=0.5\n",
      "        The exponent for inverse scaling learning rate [default 0.5].\n",
      "\n",
      "    early_stopping : bool, default=False\n",
      "        Whether to use early stopping to terminate training when validation\n",
      "        score is not improving. If set to True, it will automatically set aside\n",
      "        a stratified fraction of training data as validation and terminate\n",
      "        training when validation score returned by the `score` method is not\n",
      "        improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'early_stopping' option\n",
      "\n",
      "    validation_fraction : float, default=0.1\n",
      "        The proportion of training data to set aside as validation set for\n",
      "        early stopping. Must be between 0 and 1.\n",
      "        Only used if `early_stopping` is True.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'validation_fraction' option\n",
      "\n",
      "    n_iter_no_change : int, default=5\n",
      "        Number of iterations with no improvement to wait before stopping\n",
      "        fitting.\n",
      "        Convergence is checked against the training loss or the\n",
      "        validation loss depending on the `early_stopping` parameter.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "            Added 'n_iter_no_change' option\n",
      "\n",
      "    class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "        Preset for the class_weight fit parameter.\n",
      "\n",
      "        Weights associated with classes. If not given, all classes\n",
      "        are supposed to have weight one.\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to True, reuse the solution of the previous call to fit as\n",
      "        initialization, otherwise, just erase the previous solution.\n",
      "        See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "        Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "        result in a different solution than when calling fit a single time\n",
      "        because of the way the data is shuffled.\n",
      "        If a dynamic learning rate is used, the learning rate is adapted\n",
      "        depending on the number of samples already seen. Calling ``fit`` resets\n",
      "        this counter, while ``partial_fit`` will result in increasing the\n",
      "        existing counter.\n",
      "\n",
      "    average : bool or int, default=False\n",
      "        When set to True, computes the averaged SGD weights accross all\n",
      "        updates and stores the result in the ``coef_`` attribute. If set to\n",
      "        an int greater than 1, averaging will begin once the total number of\n",
      "        samples seen reaches `average`. So ``average=10`` will begin\n",
      "        averaging after seeing 10 samples.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    coef_ : ndarray of shape (1, n_features) if n_classes == 2 else \\\n",
      "            (n_classes, n_features)\n",
      "        Weights assigned to the features.\n",
      "\n",
      "    intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "        Constants in decision function.\n",
      "\n",
      "    n_iter_ : int\n",
      "        The actual number of iterations before reaching the stopping criterion.\n",
      "        For multiclass fits, it is the maximum over every binary fit.\n",
      "\n",
      "    loss_function_ : concrete ``LossFunction``\n",
      "\n",
      "    classes_ : array of shape (n_classes,)\n",
      "\n",
      "    t_ : int\n",
      "        Number of weight updates performed during training.\n",
      "        Same as ``(n_iter_ * n_samples)``.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.svm.LinearSVC : Linear support vector classification.\n",
      "    LogisticRegression : Logistic regression.\n",
      "    Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "        penalty=None)``.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.linear_model import SGDClassifier\n",
      "    >>> from sklearn.preprocessing import StandardScaler\n",
      "    >>> from sklearn.pipeline import make_pipeline\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "    >>> Y = np.array([1, 1, 2, 2])\n",
      "    >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "    >>> clf = make_pipeline(StandardScaler(),\n",
      "    ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "    >>> clf.fit(X, Y)\n",
      "    Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                    ('sgdclassifier', SGDClassifier())])\n",
      "    >>> print(clf.predict([[-0.8, -1]]))\n",
      "    [1]\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"hinge\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEFAULT_EPSILON\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"optimal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpower_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"modified_huber\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"probability estimates are not available for\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                                 \u001b[1;34m\" loss=%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Probability estimates.\n",
      "\n",
      "        This method is only available for log loss and modified Huber loss.\n",
      "\n",
      "        Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "        estimates by simple normalization, as recommended by Zadrozny and\n",
      "        Elkan.\n",
      "\n",
      "        Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "        it is necessary to perform proper probability calibration by wrapping\n",
      "        the classifier with\n",
      "        :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "            Input data for prediction.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        ndarray of shape (n_samples, n_classes)\n",
      "            Returns the probability of the sample for each class in the model,\n",
      "            where classes are ordered as they are in `self.classes_`.\n",
      "\n",
      "        References\n",
      "        ----------\n",
      "        Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "        probability estimates\", SIGKDD'02,\n",
      "        http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      "\n",
      "        The justification for the formula in the loss=\"modified_huber\"\n",
      "        case is in the appendix B in:\n",
      "        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"log\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"modified_huber\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mprob\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mprob\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# the above might assign zero to all classes, which doesn't\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# normalize neatly; work around this to produce uniform\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# probabilities\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mall_zero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprob_sum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[0mprob_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_zero\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mprob\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mprob_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predict_(log_)proba only supported when\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                                      \u001b[1;34m\" loss='log' or loss='modified_huber' \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                                      \u001b[1;34m\"(%r given)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mpredict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Log of probability estimates.\n",
      "\n",
      "        This method is only available for log loss and modified Huber loss.\n",
      "\n",
      "        When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "        and ones, so taking the logarithm is not possible.\n",
      "\n",
      "        See ``predict_proba`` for details.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input data for prediction.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        T : array-like, shape (n_samples, n_classes)\n",
      "            Returns the log-probability of the sample for each class in the\n",
      "            model, where classes are ordered as they are in\n",
      "            `self.classes_`.\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_log_proba\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_predict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'_xfail_checks'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'check_sample_weights_invariance'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'zero sample_weight is not equivalent to removing samples'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\aminou ahmad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "0ac051481fb55a289e6d3811842eb52f8d8778857a84b9402ebfcfeb0effd68f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}